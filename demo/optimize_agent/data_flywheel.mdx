---
title: 'Data Flywheel'
---

In the last section we created test sets of varying sizes,
ready to evaluate our agent.
So, it's finally time to start our data flywheel spinning!

<img class="dark-light" width="100%" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/refs/heads/main/img/externally_linked/flywheel-cropd-2mb-dark.gif"/>

In pseudo-code,
the general process for optimizing an LLM agent is quite straightforward:

```
1  Create simplest possible agent ğŸ¤–
2  While True:
3      Create/expand unit tests (evals) ğŸ—‚ï¸
4      While run(tests) failing: ğŸ§ª
5          Analyze failures, understand the root cause ğŸ”
6          Vary system prompt, in-context examples, tools etc. to rectify ğŸ”€
7      [Optional] Beta test with users, find more failures ğŸš¦
```

Firstly, let's activate the `MarkingAssistant` project.

```python
unify.activate("MarkingAssistant")
```

Let's also set a new context `Evals`,
where we'll store all of our evaluation runs.

```python
unify.set_context("Evals")
```

Great, we can now dive into the first step of the flywheel! ğŸ¤¿
