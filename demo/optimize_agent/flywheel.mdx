---
title: 'Flywheel'
---

In the last section we created test sets of varying sizes,
ready to evaluate our agent.
So, it's finally time to start our data flywheel spinning!
The general process for optimizing an LLM agent is quite straightforward:

```
1  Create simplest possible agent ğŸ¤–
2  While True:
3      Create/expand unit tests (evals) ğŸ—‚ï¸
4      While run(tests) failing: ğŸ§ª
5          Analyze failures, understand the root cause ğŸ”
6          Vary system prompt, in-context examples, tools etc. to rectify ğŸ”€
7      [Optional] Beta test with users, find more failures ğŸš¦
```

Firstly, let's activate the `MarkingAssistant` project.

```python
unify.activate("MarkingAssistant")
```

Let's also set a new context `Evals`,
where we'll store all of our evaluation runs.

```python
unify.set_context("Evals")
```

Great, we can now dive into the first step of the flywheel! ğŸ¤¿
