{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I9lo9vMW5GB"
      },
      "source": [
        "# Accelerating MMPreTrain models with JAX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBvR4DRxW7TK"
      },
      "source": [
        "Accelerate your MMPreTrain models by converting them to JAX for faster inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OidaQvRaD3a"
      },
      "source": [
        "Installations  \n",
        "\n",
        "Make sure you run this demo with GPU enabled!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BX7ZSGstXcP5"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q openmim && mim install -q \"mmpretrain>=1.0.0rc8\"\n",
        "!pip install -q ivy\n",
        "!pip install -q dm-haiku"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1GtTjJOdUgG"
      },
      "source": [
        "Let's now import Ivy and the libraries we'll use in this example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "29c5UttUsK17"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "jax.devices()\n",
        "import ivy\n",
        "ivy.set_default_device(\"gpu:0\")\n",
        "import torch\n",
        "import requests\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "import torchvision\n",
        "from mmpretrain import get_model, list_models\n",
        "from mmengine import ConfigDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN0oSCTwdkKg"
      },
      "source": [
        "Sanity check to make sure checkpoint name is correct against mmpretrain's [model zoo](https://mmpretrain.readthedocs.io/en/latest/modelzoo_statistics.html#pretrained-models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJvIzkkovaLw",
        "outputId": "92035e8b-a2bc-4fed-eb7c-735417188160"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['convnext-tiny_32xb128-noema_in1k']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint_name = \"convnext-tiny_32xb128-noema_in1k\"\n",
        "list_models(checkpoint_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6EF-otSdaxn"
      },
      "source": [
        "Now we can load the ConvNext model from OpenMMLab's mmpretrain library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fl2RJ_KlsNy2"
      },
      "outputs": [],
      "source": [
        "jax.config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "model = get_model(checkpoint_name, pretrained=True, device='cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9orwOx9eDhx"
      },
      "source": [
        "We will also need a sample image to pass during tracing, so let's use the appropriate transforms to get the corresponding torch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YXMd5jrYaD3b"
      },
      "outputs": [],
      "source": [
        "def get_scale(cfg):\n",
        "    if type(cfg) == ConfigDict:\n",
        "        if cfg.get('type', False) and cfg.get('scale', False):\n",
        "            return cfg['scale']\n",
        "        else:\n",
        "            for k in cfg.keys():\n",
        "                input_shape = get_scale(cfg[k])\n",
        "                if input_shape:\n",
        "                    return input_shape\n",
        "    elif type(cfg) == list:\n",
        "        for block in cfg:\n",
        "            input_shape = get_scale(block)\n",
        "            if input_shape:\n",
        "                return input_shape\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sd_ywJE77Pwp"
      },
      "outputs": [],
      "source": [
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "input_shape = get_scale(model._config.train_pipeline)\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((input_shape, input_shape)),\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "tensor_image = transform(image).unsqueeze(0).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dapWhFdRegVG"
      },
      "source": [
        "And finally, let's transpile the model to haiku!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJGzJLmYuu-a"
      },
      "outputs": [],
      "source": [
        "transpiled_graph = ivy.transpile(model, to=\"haiku\", args=(tensor_image,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqUkwEhEemfX"
      },
      "source": [
        "After transpiling our model, we can see what's the improvement in runtime efficiency like. For this let's compile the original PyTorch model using `torch.compile`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AZVq72BQ7lHV"
      },
      "outputs": [],
      "source": [
        "tensor_image = transform(image).unsqueeze(0).to(\"cuda\")\n",
        "\n",
        "def _f(args):\n",
        "  return model(args)\n",
        "\n",
        "comp_model = torch.compile(_f)\n",
        "_ = comp_model(tensor_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg1o9T-B9aIr"
      },
      "source": [
        "Let's now do the equivalent transformation in our new haiku model by using JAX just in time compilation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YQk3gbihv483"
      },
      "outputs": [],
      "source": [
        "tensor_image = transform(image).unsqueeze(0).to(\"cuda\")\n",
        "np_image = tensor_image.detach().cpu().numpy()\n",
        "jax_image = jax.device_put(jax.numpy.asarray(np_image), device=jax.devices()[0])\n",
        "\n",
        "import haiku as hk\n",
        "\n",
        "def _forward(args):\n",
        "  module = transpiled_graph()\n",
        "  return module(args)\n",
        "\n",
        "rng_key = jax.random.PRNGKey(42)\n",
        "jax_mlp_forward = hk.transform(_forward)\n",
        "params = jax_mlp_forward.init(rng=rng_key, args=jax_image)\n",
        "apply = jax.jit(jax_mlp_forward.apply)\n",
        "_ = apply(params, None, jax_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ulQ5z1n9SuR"
      },
      "source": [
        "Now that we have both models optimized, let's see how their runtime speeds compare to each other!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LOd86nDv0uW",
        "outputId": "2a502158-f7ba-4b69-ab25-0653b17ef090"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8.06 ms Â± 2.7 ms per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit comp_model(tensor_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7r02dlwv6ce",
        "outputId": "6dd39678-6577-41e2-e539-4f564fb0faeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.08 ms Â± 13.2 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit apply(params, None, jax_image).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR2BAWZC-hvh"
      },
      "source": [
        "As expected, we have made the model significantly faster with just one line of code! Latency gets even better on a V100 GPU, where we can get up to a 2-3x increase in execution speed! ðŸš€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGu1iznHr8LI"
      },
      "source": [
        "Finally, as a sanity check, let's load a different image and make sure that the results are the same in both models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6aMaMbbr8LI",
        "outputId": "feb0dd6f-5ca9-4b5e-f920-6cc015c30062"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch call took: 6.66ms\n",
            "Jax call took: 2.53ms\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "url = \"http://images.cocodataset.org/train2017/000000283921.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "tensor_image = transform(image).unsqueeze(0).to(\"cuda\")\n",
        "np_image = tensor_image.detach().cpu().numpy()\n",
        "jax_image = jax.device_put(jax.numpy.asarray(np_image), device=jax.devices()[0])\n",
        "\n",
        "st = time.perf_counter()\n",
        "out_torch = comp_model(tensor_image)\n",
        "et = time.perf_counter()\n",
        "print(f'Torch call took: {(et - st) * 1000:.2f}ms')\n",
        "\n",
        "st = time.perf_counter()\n",
        "out_jax = apply(params, None, jax_image)\n",
        "et = time.perf_counter()\n",
        "print(f'Jax call took: {(et - st) * 1000:.2f}ms')\n",
        "\n",
        "print(np.allclose(out_torch.detach().cpu().numpy(), out_jax, atol=1e-4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChfnzP1rfdC4"
      },
      "source": [
        "That's pretty much it! The results from both models are the same, but we have achieved a solid speed up by using Ivy's transpiler to convert the model to JAX!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
