
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Benchmarks &#8212; Unify Documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=7c465b21" />
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Inter:100,200,300,regular,500,600,700,800,900" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=3ce10a4d"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=4ea706d9"></script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-QP5BET66XH"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-QP5BET66XH');
            </script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-QP5BET66XH');
            </script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'concepts/benchmarks';</script>
    <link rel="icon" href="https://github.com/unifyai/unifyai.github.io/blob/main/img/externally_linked/ivy_logo_only.png?raw=true"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Routing" href="routing.html" />
    <link rel="prev" title="Model Endpoints" href="endpoints.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar"><div id="unify-navbar"></div>
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item">
<nav class="navbar-nav">
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../index.html">
                        Welcome to Unify!
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../api/unify_api.html">
                        Universal API
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../api/benchmarks.html">
                        Benchmarking
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../api/router.html">
                        Routing
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../demos/unify.html">
                        Python Package Examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../demos/langchain.html">
                        LangChain Examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../demos/llamaindex.html">
                        LlamaIndex Examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../interfaces/connecting_stack.html">
                        Connecting your stack
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../interfaces/running_benchmarks.html">
                        Benchmarking endpoints
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../interfaces/building_router.html">
                        Building a custom router
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="endpoints.html">
                        Model Endpoints
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="#">
                        Benchmarks
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="routing.html">
                        Routing
                      </a>
                    </li>
                
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><nav class="bd-docs-nav bd-links"
  aria-label="Section Navigation">
    <div class="bd-toc-item navbar-nav">
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Welcome to Unify!</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../api/unify_api.html">Universal API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/benchmarks.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/router.html">Routing</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Demos</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../demos/unify.html">Python Package Examples</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../demos/demos/Unify/ChatBot/ChatBot.html">Build a ChatBot</a></li>


<li class="toctree-l2"><a class="reference internal" href="../demos/demos/Unify/AsyncVsSync/AsyncVsSync.html">Synchronous VS Asynchronous Clients</a></li>

<li class="toctree-l2"><a class="reference internal" href="../demos/demos/Unify/LLM-Wars/README.html">LLM-Wars</a></li>
<li class="toctree-l2"><a class="reference internal" href="../demos/demos/Unify/SemanticRouter/README.html">Semantic Router</a></li>
<li class="toctree-l2"><a class="reference internal" href="../demos/demos/Unify/Chatbot_Arena/README.html">Chatbot Arena</a></li>
<li class="toctree-l2"><a class="reference internal" href="../demos/demos/Unify/LLM_Debate/README.html">AI Debate App</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../demos/langchain.html">LangChain Examples</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../demos/demos/LangChain/RAG_playground/README.html">RAG Playground 🛝</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../demos/llamaindex.html">LlamaIndex Examples</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../demos/demos/LlamaIndex/RAGPlayground/README.html">RAG Playground</a></li>
<li class="toctree-l2"><a class="reference internal" href="../demos/demos/LlamaIndex/BasicUsage/unify.html">LlamaIndex</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Interfaces</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../interfaces/connecting_stack.html">Connecting your stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../interfaces/running_benchmarks.html">Benchmarking endpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../interfaces/building_router.html">Building a custom router</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Concepts</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="endpoints.html">Model Endpoints</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="routing.html">Routing</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">Benchmarks</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="benchmarks">
<h1>Benchmarks<a class="headerlink" href="#benchmarks" title="Link to this heading">#</a></h1>
<p>In this section, we explain our process for benchmarking LLM endpoints. We discuss quality and runtime benchmarks separately.</p>
<section id="quality-benchmarks">
<h2>Quality Benchmarks<a class="headerlink" href="#quality-benchmarks" title="Link to this heading">#</a></h2>
<p>Finding the best LLM(s) for a given application can be challenging. The performance of a model can vary significantly depending on the task, dataset, and evaluation metrics used. Existing benchmarks attempt to compare models based on standardized approaches, but biases inevitably creep in as models learn to do well on these targeted assessments.</p>
<p>Practically, the LLM community still heavily relies on testing models manually to build an intuition around their expected behavior for a given use-case. While this generally works better, hand-crafted testing isn’t sustainable as one’s needs evolve and new LLMs emerge at a rapid pace.
Our LLM assessment pipeline is based on the method outlined below.</p>
<section id="design-principles">
<h3>Design Principles<a class="headerlink" href="#design-principles" title="Link to this heading">#</a></h3>
<p>Our quality benchmarks are based on a set of guiding principles. Specifically, we strive to make our pipeline:</p>
<ul class="simple">
<li><p><strong>Systematized:</strong> A rigorous benchmarking pipeline should be standardized across assessments, repeatable, and scalable. We make sure to benchmark all LLMs identically to with a well-defined approach we outline in the next passage.</p></li>
<li><p><strong>Task-centric:</strong> Models perform differently on various tasks. Some might do better at coding, others are well suited for summarizing content, etc. These broad task categories can also be refined into specific subtasks. For e.g summarizing technical content to generate product documentation is radically different from summarizing news. This should be reflected in assessments. For this reason, we allow you to upload your custom prompt dataset, that you believe reflects the intended task, to use as a reference for running benchmarks.</p></li>
<li><p><strong>Customizable:</strong> Assessments should reflect the unique needs of the assessor. Depending on your application requirements, you may need to strictly include / exclude some models from the benchmarks. We try to strike a balance between standardization and modularity such that you can run the benchmarks that are relevant to your needs.</p></li>
</ul>
</section>
<section id="methodology">
<h3>Methodology<a class="headerlink" href="#methodology" title="Link to this heading">#</a></h3>
<section id="overview">
<h4>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h4>
<p>We benchmark models using the LLM-as-a-judge approach. This relies on using a powerful language model to generate assessments on the outputs of other models, using a standard reviewing procedure. LLM-as-a-judge is sometimes used to run experiments at scale when generating human assessments isn’t an option or to avoid introducing human biases.</p>
<p>Given a dataset of user prompts, each prompt is sent to all endpoints to generate an output. Then, we ask GPT-4 to review each output and give a final assessment based on how helpful and accurate the response is relative to either (a) the user prompt, in the case of unlabelled datasets, or (b) the prompt and the reference answer, in the case of labelled datasets.</p>
</section>
<section id="scoring">
<h4>Scoring<a class="headerlink" href="#scoring" title="Link to this heading">#</a></h4>
<p>The assessor LLM reviews the output of an endpoint which it categorizes as <code class="code docutils literal notranslate"><span class="pre">irrelevant</span></code>, <code class="code docutils literal notranslate"><span class="pre">bad</span></code>, <code class="code docutils literal notranslate"><span class="pre">satisfactory</span></code>, <code class="code docutils literal notranslate"><span class="pre">very</span> <span class="pre">good</span></code>, or <code class="code docutils literal notranslate"><span class="pre">excellent</span></code>. Each of these labels is then mapped to a numeric score ranging from 0.0 to 1.0. We repeat the same proces for all prompts in the dataset to get the endpoint’s performance score on each prompt. The overall endpoint’s score is then the average of these prompt-specific scores.</p>
</section>
<section id="visualizing-results">
<h4>Visualizing Results<a class="headerlink" href="#visualizing-results" title="Link to this heading">#</a></h4>
<p>In addition to the list of model scores, we also compute runtime performance for the endpoint (as explained in the section below). Doing so allows us to plot the quality performance versus runtime to assess the quality-to-performance of the endpoints, instead of relying on the quality scores alone.</p>
<a class="reference internal image-reference" href="../_images/console_dashboard.png"><img alt="Console Dashboard." class="align-center" src="../_images/console_dashboard.png" style="width: 650px;" /></a>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Because quality scores are model-specific, they are the same across the different endpoints exposed for a given model. As a result, all the endpoints for a model will plot horizontally at the same quality level, with only the runtime metric setting them apart.</p>
</div>
</section>
</section>
<section id="considerations-and-limitations">
<h3>Considerations and Limitations<a class="headerlink" href="#considerations-and-limitations" title="Link to this heading">#</a></h3>
<p>Despite having a well-defined benchmarking approach, it also inevitably comes with its own issues. Using an LLM to judge outputs may introduce a different kind of bias through the data used to train the assessor model. We are currently looking at ways to mitigate this with more diversified and / or customized judge LLM selection.</p>
</section>
</section>
<section id="runtime-benchmarks">
<h2>Runtime Benchmarks<a class="headerlink" href="#runtime-benchmarks" title="Link to this heading">#</a></h2>
<p>Finding the best model(s) for a task is just the first step to optimize LLM pipelines. Given the plethora of endpoint providers offering the same models, true optimization requires considering performance discrepancies across endpoints and time.</p>
<p>Because this is a complex decision, it needs to be made based on data. For this data to be reliable, it should also result from transparent and objective measurements, which we outline in this below.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Our benchmarking code is openly available in <a class="reference external" href="https://github.com/unifyai/aibench-llm-endpoints">this repository</a>.</p>
</div>
<section id="id1">
<h3>Design Principles<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>Our runtime benchmarks are based on a set of guiding principles. Specifically, we believe benchmarks should be:</p>
<ul class="simple">
<li><p><strong>Community-driven:</strong> We invite everyone to audit or improve the logic and the code. We are building these benchmarks for the community, so contributions and discussions around them are more than welcome!</p></li>
<li><p><strong>User-centric:</strong> External factors (e.g. how different providers set up their infrastructure) may impact measurements. Nevertheless, our benchmarks are not designed to gauge performance in controlled environments. Rather, we aime to measure performance as experienced by the end-user who, ultimately, is subject to the same distortions.</p></li>
<li><p><strong>Model and Provider-agnostic:</strong> While some metrics are more relevant to certain scenarios (e.g. cold start time in model endpoints that scale to zero), we try to make as few assumptions as possible on the providers or technologies being benchmarked. We only assume that endpoints take a string as the input and return a streaming response.</p></li>
</ul>
</section>
<section id="id2">
<h3>Methodology<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<section id="tokenizer">
<h4>Tokenizer<a class="headerlink" href="#tokenizer" title="Link to this heading">#</a></h4>
<p>To avoid biases towards any model-specific tokenizer, we calculate all metrics using the same tokenizer across different models. We have chosen the <cite>cl100k_base</cite> tokenizer from OpenAI’s <a class="reference external" href="https://github.com/openai/tiktoken">tiktoken</a> library for this since it’s MIT licensed and already widely adopted by the community.</p>
</section>
<section id="inputs-and-outputs">
<h4>Inputs and Outputs<a class="headerlink" href="#inputs-and-outputs" title="Link to this heading">#</a></h4>
<p>To fairly assess optimizations such as speculative decoding, we use real text as the input and avoid using randomly generated data. The length of the input affects prefill time and therefore can affect the responsiveness of the system. To account for this, we run the benchmark with two input regimes.</p>
<ul class="simple">
<li><p>Short inputs: Using sentences with an average length of 200 tokens and a standard deviation of 20.</p></li>
<li><p>Long inputs: Using sentences with an average length of 1000 tokens and a standard deviation of 100.</p></li>
</ul>
<p>To build these clusters, we programmatically select sentences from <a class="reference external" href="https://huggingface.co/datasets/bookcorpus">BookCorpus</a> and create two subsets of it. For instruct/chat models to answer appropriately and ensure a long enough response, we preface each prompt with <code class="code docutils literal notranslate"><span class="pre">Repeat</span> <span class="pre">the</span> <span class="pre">following</span> <span class="pre">lines</span> <span class="pre">&lt;#&gt;</span> <span class="pre">times</span> <span class="pre">without</span> <span class="pre">generating</span> <span class="pre">the</span> <span class="pre">EOS</span> <span class="pre">token</span> <span class="pre">earlier</span> <span class="pre">than</span> <span class="pre">that</span></code>, where <code class="code docutils literal notranslate"><span class="pre">&lt;#&gt;</span></code> is randomly sampled.</p>
<p>For the outputs, we use randomized discrete values from the same distributions (i.e. N(200, 20) for short inputs and N(1000, 100) for long ones) to cap the number of tokens in the output. This ensures variable output length, which is necessary to consider algorithms such as Paged Attention or Dynamic Batching.</p>
<p>When running one benchmark across different endpoints, we seed each runner with the same initial value, so that the inputs are the same for all endpoints.</p>
</section>
<section id="computation">
<h4>Computation<a class="headerlink" href="#computation" title="Link to this heading">#</a></h4>
<p>To execute the benchmarks, we run three processes periodically from three different regions: <strong>Hong Kong, Belgium and Iowa</strong>. Each one of these processes is triggered every three hours and benchmarks every available endpoint.</p>
<p>Accounting for the different input policies, we run a total of 4 benchmarks for each endpoint every time a region benchmark is triggered.</p>
</section>
<section id="metrics">
<h4>Metrics<a class="headerlink" href="#metrics" title="Link to this heading">#</a></h4>
<p>Several key metrics are captured and calculated during the benchmarking process:</p>
<ul class="simple">
<li><p><strong>Time to First Token (TTFT):</strong> Time between request initiation and the arrival of the first streaming response packet. TTFT directly reflects the prompt processing speed, offering insights into the efficiency of the model’s initial response. A lower TTFT signifies quicker engagement, which is crucial for applications that require dynamic interactions or real-time feedback.</p></li>
<li><p><strong>End to End Latency:</strong> Time between request initiation and the arrival of the final packet in the streaming response. This metric provides a holistic view of the response time, including processing and transmission.</p></li>
<li><p><strong>Inter Token Latency (ITL):</strong> Average time between consecutive tokens in the response. We compute this as <code class="code docutils literal notranslate"><span class="pre">(End</span> <span class="pre">to</span> <span class="pre">End</span> <span class="pre">Latency)</span> <span class="pre">/</span> <span class="pre">(Output</span> <span class="pre">Tokens</span> <span class="pre">-</span> <span class="pre">1)</span></code>.  ITL provides valuable information about the pacing of token generation and the overall temporal dynamics within the model’s output. As expected, a lower ITL signifies a more cohesive and fluid generation of tokens, which contributes to a more seamless and human-like interaction with the model.</p></li>
<li><p><strong>Number of Output Tokens per Second:</strong> Relation between the number of tokens generated and the time taken. We don’t consider the TTFT here, so this is equivalent to <code class="code docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">ITL</span></code>. In this case, a higher Number of Output Tokens per Second means a faster and more productive model output. It’s important to note that this is <strong>not</strong> a measurement of the throughput of the inference server since it doesn’t account for batched inputs.</p></li>
<li><p><strong>Cold Start:</strong> Time taken for a server to boot up in environments where the number of active instances can get to zero. We consider a threshold of 15 seconds. What this means is that we do an initial “dumb” request to the endpoint and record its TTFT. If this TTFT is greater than 15 seconds, we measure the time it takes to get the second token. If the ratio between the TTFT and first ITL measurements is at least 10:1, we consider the TTFT to be Cold Start time. Once this process has finished. We start the benchmark process in the warmed-up instance. This metric reflects the time it takes for the system to be ready for processing requests, rendering it essential for users relying on prompt and consistent model responses, allowing you to account for any potential initialization delays in the responses and ensuring a more accurate expectation of the model’s responsiveness.</p></li>
<li><p><strong>Cost</strong>: Last but not least, we present information about the cost of querying the model. This is usually different for the input tokens and the response tokens, so it can be beneficial to choose different models depending on the end task. As an example, to summarize a document, a provider with lower price in the input tokens would be better, even if it comes with a slightly higher price in the output. On the other hand, if you want to generate long-format content, a provider with a lower price per generated token will be the most appropriate option.</p></li>
</ul>
</section>
<section id="data-presentation">
<h4>Data Presentation<a class="headerlink" href="#data-presentation" title="Link to this heading">#</a></h4>
<p>When aggregating metrics, particularly in benchmark regimes with multiple concurrent requests, we calculate and present the P90 (90th percentile) value from the set of measurements. We choose the P90 to reduce the influence of extreme values and provide a reliable snapshot of the model’s performance.</p>
<p>When applicable, aggregated data is shown both in the plots and the benchmark tables.</p>
<a class="reference internal image-reference" href="../_images/benchmarks_model_page.png"><img alt="Benchmarks Model Page." class="align-center" src="../_images/benchmarks_model_page.png" style="width: 650px;" /></a>
<p>Additionally, we also include a MA5 view (Moving Average of the last 5 measurements) in the graphs. This smoothing technique helps mitigate short-term fluctuations and should provide a clearer trend representation over time.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some cases, you will find <code class="code docutils literal notranslate"><span class="pre">Not</span> <span class="pre">computed</span></code> instead of a value, or even a <code class="code docutils literal notranslate"><span class="pre">No</span> <span class="pre">metrics</span> <span class="pre">are</span> <span class="pre">available</span> <span class="pre">yet</span></code> message instead of the benchmark data. This is typically due to an internal issue or a rate limit, which we’ll be quickly fixing.</p>
</div>
</section>
</section>
<section id="id3">
<h3>Considerations and Limitations<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>We try to tackle some of the more significant limitations of benchmarking inference endpoints. For example, network latency, by running the benchmarks in different regions; or unreliable point-measurements, by continuously benchmarking the endpoints and plotting their trends over time.</p>
<p>However, there are still some relevant considerations to have in mind. Our methodology at the moment is solely focused on performance, which means that we don’t look at the output of the models.</p>
<p>Nonetheless, even accounting for the public-facing nature of these endpoints (no gibberish allowed!), there might be some implementation differences that affect the output quality, such as quantization/compression of the models, different context window sizes, or different speculative decoding models, among others. We are working towards mitigating this as well, so stay tuned!</p>
</section>
</section>
<section id="round-up">
<h2>Round Up<a class="headerlink" href="#round-up" title="Link to this heading">#</a></h2>
<p>You are now familiar with how we run our benchmarks. Next, you can explore how to <a class="reference external" href="https://unify.ai/docs/interfaces/running_benchmarks.html">use the benchmarks, or run your own</a> through the benchmarks interface!</p>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="endpoints.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Model Endpoints</p>
      </div>
    </a>
    <a class="right-next"
       href="routing.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Routing</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quality-benchmarks">Quality Benchmarks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#design-principles">Design Principles</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#methodology">Methodology</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scoring">Scoring</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-results">Visualizing Results</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#considerations-and-limitations">Considerations and Limitations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#runtime-benchmarks">Runtime Benchmarks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Design Principles</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Methodology</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenizer">Tokenizer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#inputs-and-outputs">Inputs and Outputs</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computation">Computation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics">Metrics</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-presentation">Data Presentation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Considerations and Limitations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#round-up">Round Up</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
<script src="https://cdn.saas.unify.ai/js/unify-components-shared.js"></script>
<script src="https://cdn.saas.unify.ai/js/unify-components.js"></script>
<div id="navbar-contents" style="display: none;">
  
  <div class="navbar-header-items__end">
    
    <div class="navbar-item navbar-persistent--container">
      

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
    </div>
    
    
    <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
    
  </div>
  
</div>
<script>
  const html = `<div></div>`;
  const toc = `<nav class="bd-docs-nav bd-links"
  aria-label="Section Navigation">
    <div class="bd-toc-item navbar-nav">
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Welcome to Unify!</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../api/unify_api.html">Universal API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/benchmarks.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/router.html">Routing</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Demos</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../demos/unify.html">Python Package Examples</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../demos/demos/Unify/ChatBot/ChatBot.html">Build a ChatBot</a></li>


<li class="toctree-l2"><a class="reference internal" href="../demos/demos/Unify/AsyncVsSync/AsyncVsSync.html">Synchronous VS Asynchronous Clients</a></li>

<li class="toctree-l2"><a class="reference internal" href="../demos/demos/Unify/LLM-Wars/README.html">LLM-Wars</a></li>
<li class="toctree-l2"><a class="reference internal" href="../demos/demos/Unify/SemanticRouter/README.html">Semantic Router</a></li>
<li class="toctree-l2"><a class="reference internal" href="../demos/demos/Unify/Chatbot_Arena/README.html">Chatbot Arena</a></li>
<li class="toctree-l2"><a class="reference internal" href="../demos/demos/Unify/LLM_Debate/README.html">AI Debate App</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../demos/langchain.html">LangChain Examples</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../demos/demos/LangChain/RAG_playground/README.html">RAG Playground 🛝</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../demos/llamaindex.html">LlamaIndex Examples</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../demos/demos/LlamaIndex/RAGPlayground/README.html">RAG Playground</a></li>
<li class="toctree-l2"><a class="reference internal" href="../demos/demos/LlamaIndex/BasicUsage/unify.html">LlamaIndex</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Interfaces</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../interfaces/connecting_stack.html">Connecting your stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../interfaces/running_benchmarks.html">Benchmarking endpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../interfaces/building_router.html">Building a custom router</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Concepts</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="endpoints.html">Model Endpoints</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="routing.html">Routing</a></li>
</ul>

    </div>
</nav>`;
  initializeNavbar(html, true, toc);
  Promise.all([waitForElm("unify-navbar-end"), waitForElm("unify-mobile-navbar-end")]).then(([target, mobileTarget]) => {
    for (let elem of document.getElementById("navbar-contents").children) {
      target.children[0].appendChild(elem.cloneNode(true));
      mobileTarget.children[0].appendChild(elem.cloneNode(true));
    }
    document.getElementById("navbar-contents").remove();
    const scripts = `
  &lt!-- Scripts loaded after &ltbody&gt so the DOM is not blocked --&gt
  &ltscript src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"&gt&lt/script&gt
&ltscript src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"&gt&lt/script&gt
`.replace(/&lt/g, "<").replace(/&gt/g, ">");
    const divFragment = document.createRange().createContextualFragment(scripts);
    document.body.prepend(divFragment);
  });
</script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2020-2023, Unify.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.5.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.2.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>