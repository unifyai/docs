---
title: 'Evaluators'
---


On a high level, an evaluator is *basically* any function which receives a prompt,
receives optional extra fields corresponding to the prompt, receives a *response* to
the prompt, and then returns a score evaluating the response, that's it!

There is a bit more nuance to the exact types used in the implementation,
but conceptually, this is all an evaluator is.
Let's dive in and create a simple evaluator! ðŸ§‘â€ðŸ’»

## Creation

To begin with, let's assume we have the following dataset to evaluate:

```python
import unify
from typing import Union, Type, Dict
system_msg = "Answer the following maths question, " \
             "returning only the numeric answer, and nothing else."
dataset = unify.Dataset(
    [unify.Prompt(q, system_message=system_msg) for q in ["1 + 3", "4 + 7", "6 + 5"]]
)
```

Before creating an `Evaluator`, we need to define the scoring system, giving several
different available scores between 0-1, each with their own *description*.
We do this by subclassing `unify.Score`, and overriding the abstract `config` property.
In this case, there are only two possible outcomes, and we can implement a simple binary
score class as follows:

```python
class Binary(unify.Score):

    @property
    def config(self) -> Dict[float, str]:
        return {
            0.: "incorrect",
            1.: "correct"
        }
```

An evaluator can then be created by subclassing `unify.Evaluator`,
and overriding the abstract `class_config` property and the abstract `_evaluate` method,
as follows:

```python
class MathsEvaluator(unify.Evaluator):

    @property
    def class_config(self) -> Type[unify.Score]:
        return Binary

    def _evaluate(self, prompt: str, response: str) -> bool:
        correct_answer = eval(prompt)
        try:
            response_int = int(
                "".join([c for c in response.split(" ")[-1] if c.isdigit()])
            )
            return correct_answer == response_int
        except ValueError:
            return False
```

Let's start by simply evaluating the first item in the dataset,
using OpenAI's `gpt-4o` as the model:

```python
client = unify.Unify("gpt-4o@openai")
evaluator = MathsEvaluator()
evaluation = evaluator.evaluate(
    prompt=dataset[0],
    response=client.generate(**dataset[0].prompt.dict()),
    agent=client
)
print(evaluation)
```
```
Evaluation(
    prompt=Prompt(
        messages=[{'content': '1 + 3', 'role': 'user'}],
        frequency_penalty=None,
        logit_bias=None,
        logprobs=None,
        top_logprobs=None,
        max_completion_tokens=None,
        n=None,
        presence_penalty=None,
        response_format=None,
        seed=None,
        stop=None,
        temperature=None,
        top_p=None,
        tools=None,
        tool_choice=None,
        parallel_tool_calls=None,
        extra_headers=None,
        extra_query=None,
        extra_body=None
    ),
    response=ChatCompletion(
        id='',
        choices=[
            Choice(
                finish_reason='stop',
                index=0,
                logprobs=None,
                message=ChatCompletionMessage(
                    content='4',
                    refusal=None,
                    role='assistant',
                    function_call=None,
                    tool_calls=None
                )
            )
        ],
        created=0,
        model='',
        object='chat.completion',
        service_tier=None,
        system_fingerprint=None,
        usage=None
    ),
    agent=Unify(endpoint=gpt-4o@openai),
    score=Binary(score=(1.0, 'correct'))
)
```

Again, we can print a much more concise representation after calling
`unify.set_repr_mode("concise")`. As usual, we will assume `"concise"` mode is set
for the rest of the examples on this page:

```
Evaluation(
    prompt=Prompt(messages=[{'content': '1 + 3', 'role': 'user'}]),
    response=ChatCompletion(
        choices=[Choice(message=ChatCompletionMessage(content='4'))]
    ),
    agent=Unify(endpoint=gpt-4o@openai),
    score=Score(score=(1.0, 'correct'))
)
```

We can see that `gpt-4o` was able to get the answer correct!
Maybe AGI is around the corner after all ðŸ‘€

You'll notice that the custom implemented `_evaluate` method only receives the `prompt`
and `response`, whereas the public `evaluate` method that we called also receives the
`agent` which is being evaluated. The `agent` is also included in the `Evaluation`
instance returned. The public `evaluate` method makes use of your custom `_evaluate`
method internally, but `_evaluate` does not need to access the `agent`.

The public `evaluate` method *also* performs automatic upcasting and downcasting of the
inputs if needed, ensuring that all inputs passed to `evaluate` are subsequently cast to
the required type, as per the **type hints you've added** to your `_evaluate` method.
For example, the following runs without error, despite our custom `_evaluate`
implementation only accepting `str` inputs for `prompt` and `response`:

```python
import unify
for prompt in (unify.Datum("1 + 3"), unify.Prompt("1 + 3"), "1 + 3"):
    for response in (unify.ChatCompletion("4"), "4"):
        evaluator.evaluate(
            prompt=prompt,
            response=response,
            agent=client
        )
```

If type hints are **not provided** in your custom `_evaluate` implementation,
then no automatic casting is performed, and `prompt` will be passed to `_evaluate` as a
`unify.Prompt` instance, and `response` will be passed as a `unify.ChatCompletion`
instance.