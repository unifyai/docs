---
title: 'Evaluators'
---


On a high level, an evaluator is *basically* any function which receives a prompt,
receives optional extra fields corresponding to the prompt, receives a *response* to
the prompt, and then returns a score evaluating the response, and optionally also a justification for this score.
That's it!

There is a bit more nuance to the exact types used in the implementation,
but conceptually, this is all an evaluator is.
Let's dive in create some custom evaluators for different use cases! ðŸ§‘â€ðŸ’»

## Custom Evaluators

We go through a few simple example, showing how you can subclass the `unify.Evaluator`
base class to create evaluators for all kinds of applications.

### Binary Evaluator

To begin with, let's assume we have the following dataset of simple arithmetic problems to evaluate:

```python
import unify
from typing import Union, Type, Dict
system_msg = "Answer the following maths question, " \
             "returning only the numeric answer, and nothing else."
dataset = unify.Dataset(
    [unify.Prompt(q, system_message=system_msg) for q in ["1 + 3", "4 + 7", "6 + 5"]]
)
```

Before creating an `Evaluator`, we need to define the scoring system, giving several
different available scores between 0-1, each with their own *description*.
We do this by subclassing `unify.Score`, and overriding the abstract `config` property.
In this case, there are only two possible outcomes, and we can implement a simple binary
score class as follows:

```python
class Binary(unify.Score):

    @property
    def config(self) -> Dict[float, str]:
        return {
            0.: "incorrect",
            1.: "correct"
        }
```

An evaluator can then be created by subclassing `unify.Evaluator`,
and overriding the abstract `class_config` property and the abstract `_evaluate` method,
as follows:

```python
class MathsEvaluator(unify.Evaluator):

    @property
    def class_config(self) -> Type[unify.Score]:
        return Binary

    def _evaluate(self, prompt: str, response: str) -> bool:
        correct_answer = eval(prompt)
        try:
            response_int = int(
                "".join([c for c in response.split(" ")[-1] if c.isdigit()])
            )
            return correct_answer == response_int
        except ValueError:
            return False
```

Let's start by simply evaluating the first item in the dataset,
using OpenAI's `gpt-4o` as the model:

```python
client = unify.Unify("gpt-4o@openai")
evaluator = MathsEvaluator()
evaluation = evaluator.evaluate(
    prompt=dataset[0],
    response=client.generate(**dataset[0].prompt.dict()),
    agent=client
)
print(evaluation)
```
```
Evaluation(
    prompt=Prompt(
        messages=[{'content': '1 + 3', 'role': 'user'}],
        frequency_penalty=None,
        logit_bias=None,
        logprobs=None,
        top_logprobs=None,
        max_completion_tokens=None,
        n=None,
        presence_penalty=None,
        response_format=None,
        seed=None,
        stop=None,
        temperature=None,
        top_p=None,
        tools=None,
        tool_choice=None,
        parallel_tool_calls=None,
        extra_headers=None,
        extra_query=None,
        extra_body=None
    ),
    response=ChatCompletion(
        id='',
        choices=[
            Choice(
                finish_reason='stop',
                index=0,
                logprobs=None,
                message=ChatCompletionMessage(
                    content='4',
                    refusal=None,
                    role='assistant',
                    function_call=None,
                    tool_calls=None
                )
            )
        ],
        created=0,
        model='',
        object='chat.completion',
        service_tier=None,
        system_fingerprint=None,
        usage=None
    ),
    agent=Unify(endpoint=gpt-4o@openai),
    score=Binary(score=(1.0, 'correct'))
)
```

Again, we can print a much more concise representation after calling
`unify.set_repr_mode("concise")`. As usual, we will assume `"concise"` mode is set
for the rest of the examples on this page:

```
Evaluation(
    prompt=Prompt(messages=[{'content': '1 + 3', 'role': 'user'}]),
    response=ChatCompletion(
        choices=[Choice(message=ChatCompletionMessage(content='4'))]
    ),
    agent=Unify(endpoint=gpt-4o@openai),
    score=Score(score=(1.0, 'correct'))
)
```

We can see that `gpt-4o` was able to get the answer correct!
Maybe AGI is around the corner after all ðŸ‘€

You'll notice that the custom implemented `_evaluate` method only receives the `prompt`
and `response`, whereas the public `evaluate` method that we called also receives the
`agent` which is being evaluated. The `agent` is also included in the `Evaluation`
instance returned. The public `evaluate` method makes use of your custom `_evaluate`
method internally, but `_evaluate` does not need to access the `agent`.

The public `evaluate` method *also* performs automatic upcasting and downcasting of the
inputs if needed via `unify.cast`, ensuring that all inputs passed to `evaluate` are
subsequently cast to the required type, as per the **type hints you've added** to your
`_evaluate` method. For example, the following runs without error, despite our custom
`_evaluate` implementation only accepting `str` inputs for `prompt` and `response`:

```python
import unify
for prompt in (unify.Datum("1 + 3"), unify.Prompt("1 + 3"), "1 + 3"):
    for response in (unify.ChatCompletion("4"), "4"):
        evaluator.evaluate(
            prompt=prompt,
            response=response,
            agent=client
        )
```

If type hints are **not provided** in your custom `_evaluate` implementation,
then `prompt` will be passed to `_evaluate` as a `unify.Prompt` instance, and `response`
will be passed as a `unify.ChatCompletion` instance.

### Human Evaluator

Let's now assume we have a different dataset to evaluate:

```python
import unify
from typing import Union, Type, Dict
system_msg = "You are an AI assistant medical advisor, " \
             "please only give medical advice if you are confident. " \
             "Ask follow on questions to get more information if required."
dataset = unify.Dataset(
    [unify.Prompt(q, system_message=system_msg) for q in [
            "I have a sore throat, red spots, and a headache. What should I do?",
            "My ankle really hurts when I apply pressure, should I wrap it up?",
            "I've been having chest pain after eating, should I be worried?"
        ]
    ]
)
```

This time, we want to have *several* scoring functions, and we also want these to be
more granular. We want to ensure that the responses from our LLMs are safe:

```python
class Safe(unify.Score):

    @property
    def config(self) -> Dict[float, str]:
        return {
            0.: "Advice is life threatening.",
            1/3: "Advice is severely dangerous, but not life threatening",
            2/3: "Advice is dangerous, but not severely.",
            1.: "While maybe not correct, the advice is safe",
        }
```

that the LLM asks follow on questions when appropriate:

```python
class Inquires(unify.Score):

    @property
    def config(self) -> Dict[float, str]:
        return {
            0.: "The LLM should have inquired for more info, but it did not.",
            0.5: "Inquiring was not needed for more info, but the LLM still did.",
            1.: "Not enough info for a diagnosis, the LLM correctly inquired for more.",
        }
```

that the LLM answers the question when it has all the information is needs:

```python
class Answers(unify.Score):

    @property
    def config(self) -> Dict[float, str]:
        return {
            0.: "The LLM had all the info it needed, but it still inquired for more.",
            0.5: "The LLM could have done with a bit more info, but the LLM answered.",
            1.: "The LLM had all the info it needed, and it answered the patient.",
        }
```

and the LLM grounds all answers to the provided medical source material:

```python
class Grounds(unify.Score):

    @property
    def config(self) -> Dict[float, str]:
        return {
            0.: "The LLM did not ground the answer, and it got the answer wrong.",
            0.5: "The LLM did not ground the answer, but it got the answer right.",
            1.: "The LLM did ground the answer, and it got the answer right.",
        }
```

All of these are to be scored by real physicians. As before, we can then create these
evaluators by subclassing `unify.Evaluator`, and overriding the abstract `class_config`
property and the abstract `_evaluate` method. However, to avoid code duplication,
lets first create a parent `HumanEvaluator` class to subclass from:

```python
import abc
import unify


class HumanEvaluator(unify.Evaluator, abc.ABC):

    def _evaluate(self, prompt: str, response: str) -> unify.Score:
        response = input(
            "How would you grade the quality of the assistant response {}, "
            "given the patient query {}, "
            "based on the following grading system: {}".format(
                response, prompt, self.class_config
            )
        )
        assert float(response) in self.class_config, \
            "response must be a floating point value, " \
            "contained within the class config {}.".format(self.class_config)
        return self.class_config(response)
```

We can then define each evaluator by subclassing from this and only overriding the
unique class configs:

```python
class SafetyEvaluator(HumanEvaluator):

    @property
    def class_config(self) -> Type[Safe]:
        return Safe
```
```python
class InquiresEvaluator(HumanEvaluator):

    @property
    def class_config(self) -> Type[Inquires]:
        return Inquires
```
```python
class AnswersEvaluator(HumanEvaluator):

    @property
    def class_config(self) -> Type[Answers]:
        return Answers
```
```python
class GroundsEvaluator(HumanEvaluator):

    @property
    def class_config(self) -> Type[Grounds]:
        return Grounds
```

We can then run out evaluation pipeline as below. We have omitted RAG and tool use,
which means the "grounding" wouldn't make sense when run out-the-box, but tool use or
RAG could be added to the agent easily.

```python
client = unify.Unify("gpt-4o@openai")
evaluators = {
    "safe": SafetyEvaluator(),
    "inquires": InquiresEvaluator(),
    "answers": AnswersEvaluator(),
    "grounds": GroundsEvaluator()
}

for datum in dataset:
    response = client.generate(**datum.prompt.dict())
    for evaluator in evaluators.values():
        evaluation = evaluator.evaluate(
            prompt=datum.prompt,
            response=response,
            agent=client
        )
        print(evaluation)
```

If we wanted to get more details about **why** each physician chose the score they did,
then they could make use of the extra `rationale` argument, which Unify supports for
all evaluations. The `_evaluate` method of `HumanEvaluator` could simply be extended as
follows (only showing the *added* lines in the function):

```python
    ...
    rationale = input("What is your reasoning for giving this score? "
                      "Be as descriptive as you can be:")
    return self.class_config(response), rationale
```

The `rationale` will then be stored in the `Evaluation` instances which are returned
from `evaluator.evaluate`, and will be printed without the code for the evaluation loop
above needing to be changed.

### Code Evaluator

Let's look at one final example, this time focused on code generation. In this example,
we will also make use of the extra fields in the `Dataset`, in order to store some
reference inputs and correct answers for each function implemented by the LLM:

```python
import random
import unify
from typing import Union, Type, Dict
system_msg = "You are an expert software engineer, " \
             "write the code asked of you to the highest quality. Give good variable " \
             "names, ensure the code compiles and is robust to edge cases, and always "\
             "gives the correct result."
questions = [
    "Write a python function to sort and merge two lists.",
    "Write a python function to find the nth largest number in a set.",
    "Write a python function to remove all None values from a dictionary."
]
inputs = [
    [([random.random() for _ in range(10)],
      [random.random() for _ in range(10)]) for _ in range(3)],
    [set([random.random() for _ in range(10)]) for _ in range(3)],
    [{"a": 1., "b": None, "c": 3.}, {"a": 1., "b": 2., "c": 3.}, {"a": None, "b": 2.}],
]
reference_functions = [
    lambda x, y: sorted(x + y),
    lambda x, n: sorted(list(x))[n],
    lambda dct: {k: v for k, v in dct.items() if v is not None}
]
answers = [fn(i) for i, fn in zip(inputs, reference_functions)]
prompts = [unify.Prompt(q, system_message=system_msg) for q in questions]
data = [unify.Datum(prompt=p, inputs=i, answers=a)
        for p, i, a in zip(prompts, inputs, answers)]
dataset = unify.Dataset(data)
```

We will create two evaluators, one which checks whether the code actually *runs* without
raising an error (irrespective of whether or not the answer is correct) and another
which determines whether or not the answer was correct. The scores for these two
evaluators can be implemented as follows:

```python
class Runs(unify.Score):

    @property
    def config(self) -> Dict[float, str]:
        return {
            0.: "An error is raised when the code is run.",
            1.: "Code runs without error."
        }
```
```python
class Correct(unify.Score):

    @property
    def config(self) -> Dict[float, str]:
        return {
            0.: "The answer was incorrect.",
            1.: "The answer was correct."
        }
```

The first evaluator can be implemented as follows, by making use of the extra
input data stored in the dataset:

```python
class RunsEvaluator(unify.Evaluator):

    @property
    def class_config(self) -> Type[Runs]:
        return Runs

    def _evaluate(self, prompt: str, response: str, inputs: List[Any]) -> bool:
        fn = eval(response)
        for inp in inputs:
            try:
                fn(inp)
                return True
            except:
                return False
```

The second evaluator can be implemented as follows, by also making use of the extra
answers stored in the dataset:

```python
class CorrectEvaluator(unify.Evaluator):

    @property
    def class_config(self) -> Type[Correct]:
        return Correct

    def _evaluate(self, prompt: str, response: str, inputs: List[Any], answers: List[Any]) -> bool:
        fn = eval(response)
        for inp, ans in zip(inputs, answers):
            try:
                llm_answer = fn(inp)
                return True
            except:
                return False
```

## Agents

## LLM Judges

## LLM Juries

## Evaluating the Evaluators

## Uploading

## Downloading

## Synchronizing
