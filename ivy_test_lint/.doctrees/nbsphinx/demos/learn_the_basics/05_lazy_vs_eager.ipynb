{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy vs Eager"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand the difference between eager and lazy compilation and transpilation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ If you are running this notebook in Colab, you will have to install `Ivy` and some dependencies manually. You can do so by running the cell below ⬇️\n",
    "\n",
    "If you want to run the notebook locally but don't have Ivy installed just yet, you can check out the [Get Started section of the docs.](https://unify.ai/docs/ivy/overview/get_started.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ivy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ivy.unify`, `ivy.compile` and `ivy.transpile` can all be performed either eagerly or lazily. All previous examples have been performed **lazily**, which means that the unification, compilation, or transpilation process actually occurs during the first call of the **returned** function. \n",
    "\n",
    "This is because all three of these processes depend on function tracing, which requires function arguments to use for the tracing. Alternatively, the arguments can be provided during the `ivy.unify`, `ivy.compile` or `ivy.transpile` call itself, in which case the process is performed **eagerly**. We show some simple examples for each case below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unify"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again this simple `torch` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ivy\n",
    "import torch\n",
    "\n",
    "def normalize(x):\n",
    "    mean = torch.mean(x)\n",
    "    std = torch.std(x)\n",
    "    return torch.div(torch.sub(x, mean), std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's also create the dummy `numpy` arrays as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import NumPy\n",
    "import numpy as np\n",
    "\n",
    "# create random numpy array for testing\n",
    "x = np.random.uniform(size=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that our target framework is `tensorflow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "ivy.set_backend(\"tensorflow\")\n",
    "\n",
    "x = tf.constant(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, the function is unified **lazily**, which means the first function call will execute slowly, as this is when the unification process actually occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ivy.array([-0.54320029,  1.30825614,  1.17176882,  1.14351968, -0.98934778,\n",
       "        0.82910388, -0.89044143, -0.71881472, -0.1666683 , -1.14417601])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = ivy.unify(normalize, source=\"torch\")\n",
    "norm(x) # slow, lazy unification\n",
    "norm(x) # fast, unified on previous call"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in the following example the unification occurs **eagerly**, and both function calls will be fast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ivy.array([-0.54320029,  1.30825614,  1.17176882,  1.14351968, -0.98934778,\n",
       "        0.82910388, -0.89044143, -0.71881472, -0.1666683 , -1.14417601])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ivy.set_backend(\"tensorflow\")\n",
    "norm = ivy.unify(normalize, source=\"torch\", args=(x,))\n",
    "norm(x) # fast, unified at ivy.unify\n",
    "norm(x) # fast, unified at ivy.unify"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same is true for compiling. In the example below, the function is compiled **lazily**, which means the first function call will execute slowly, as this is when the compilation process actually occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float64, numpy=\n",
       "array([-0.54320029,  1.30825614,  1.17176882,  1.14351968, -0.98934778,\n",
       "        0.82910388, -0.89044143, -0.71881472, -0.1666683 , -1.14417601])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_comp = ivy.compile(norm)\n",
    "norm_comp(x) # slow, lazy compilation\n",
    "norm_comp(x) # fast, compiled on previous call"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in the following example the compilation occurs **eagerly**, and both function calls will be fast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float64, numpy=\n",
       "array([-0.54320029,  1.30825614,  1.17176882,  1.14351968, -0.98934778,\n",
       "        0.82910388, -0.89044143, -0.71881472, -0.1666683 , -1.14417601])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_comp = ivy.compile(norm, args=(x,))\n",
    "norm_comp(x) # fast, compiled at ivy.compile\n",
    "norm_comp(x) # fast, compiled at ivy.compile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same is true for transpiling. In the example below, the function is transpiled **lazily**, which means the first function call will execute slowly, as this is when the transpilation process actually occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float64, numpy=\n",
       "array([-0.54320029,  1.30825614,  1.17176882,  1.14351968, -0.98934778,\n",
       "        0.82910388, -0.89044143, -0.71881472, -0.1666683 , -1.14417601])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_trans = ivy.transpile(normalize, source=\"torch\", to=\"tensorflow\")\n",
    "norm_trans(x) # slow, lazy transpilation\n",
    "norm_trans(x) # fast, transpiled on previous call"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in the following example the transpilation occurs *eagerly*, and both function calls will be fast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float64, numpy=\n",
       "array([-0.54320029,  1.30825614,  1.17176882,  1.14351968, -0.98934778,\n",
       "        0.82910388, -0.89044143, -0.71881472, -0.1666683 , -1.14417601])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_trans = ivy.transpile(normalize, source=\"torch\", to=\"tensorflow\", args=(x,))\n",
    "norm_trans(x) # fast, transpiled at ivy.transpile\n",
    "norm_trans(x) # fast, transpiled at ivy.transpile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round Up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, you now know the difference between lazy vs eager execution for `ivy.unify`, `ivy.compile` and `ivy.transpile`! Next, we'll be exploring how these three functions can all be called as function decorators!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
